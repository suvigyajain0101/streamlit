{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextTranslator.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNQGFDtqRrgJJ9nBx38FgRm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-EAz_fqmDGj_"
      },
      "outputs": [],
      "source": [
        "# export langs='ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers\n",
        "! pip install sentencepiece"
      ],
      "metadata": {
        "id": "hs7TBMGPG6Va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "\n",
        "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")"
      ],
      "metadata": {
        "id": "g_cxkqyLHlwQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translator(input_sentence, input_lang_code, output_lang_code, model=model,tokenizer=tokenizer):\n",
        "  # Assign Input Language to the tokenizer\n",
        "  tokenizer.src_lang = input_lang_code\n",
        "\n",
        "  # Encode Input Sentence\n",
        "  encoded_input = tokenizer(input_sentence, return_tensors=\"pt\")\n",
        "\n",
        "  # Generate Output Tokens\n",
        "  generated_tokens = model.generate(**encoded_input, forced_bos_token_id=tokenizer.lang_code_to_id[output_lang_code])\n",
        "\n",
        "  # Convert Tokens to Sequence\n",
        "  output_sentence = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "  return output_sentence"
      ],
      "metadata": {
        "id": "y66DqKa6MQM6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lang_name_to_code = {'Arabic' : 'ar_AR', \n",
        "                 'Czech' : ' cs_CZ', \n",
        "                 'German' : ' de_DE', \n",
        "                 'English' : ' en_XX', \n",
        "                 'Spanish' : ' es_XX', \n",
        "                 'Estonian' : ' et_EE', \n",
        "                 'Finnish' : ' fi_FI', \n",
        "                 'French' : ' fr_XX', \n",
        "                 'Gujrati' : ' gu_IN', \n",
        "                 'Hindi' : ' hi_IN', \n",
        "                 'Italian' : ' it_IT', \n",
        "                 'Japanese' : ' ja_XX', \n",
        "                 'Kazakh' : ' kk_KZ', \n",
        "                 'Korean' : ' ko_KR', \n",
        "                 'Lithuanian' : ' lt_LT', \n",
        "                 'Latvian' : ' lv_LV', \n",
        "                 'Burmese' : ' my_MM', \n",
        "                 'Nepali' : ' ne_NP', \n",
        "                 'Dutch' : ' nl_XX', \n",
        "                 'Romanian' : ' ro_RO', \n",
        "                 'Russian' : ' ru_RU', \n",
        "                 'Sinhalese' : ' si_LK', \n",
        "                 'Turkish' : ' tr_TR', \n",
        "                 'Vietnamese' : ' vi_VN', \n",
        "                 'Chinese' : ' zh_CN'}\n",
        "\n",
        "lang_code_to_name = {}\n",
        "for k,v in lang_name_to_code.items():\n",
        "  lang_code_to_name[v] = k"
      ],
      "metadata": {
        "id": "EmXtDAs6JIWq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [\n",
        "          ['Yo! How are you?', 'English', 'Hindi'],\n",
        "          ['Comment ça va?', 'French', 'Chinese'],\n",
        "          ['come va?', 'Italian', 'Turkish'],\n",
        "          ['Қалайсыз?', 'Kazakh', 'Finnish'],\n",
        "          ['잘 지내고 있나요?', 'Korean', 'German']\n",
        "]"
      ],
      "metadata": {
        "id": "bLfKLhg_NMe8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inp in inputs:\n",
        "  sent = inp[0]\n",
        "  input_ln = inp[1]\n",
        "  output_ln = inp[2]\n",
        "  input_lc = lang_name_to_code[input_ln]\n",
        "  output_lc = lang_name_to_code[output_ln]\n",
        "\n",
        "  print(f'Translating from {input_ln} to {output_ln}')\n",
        "  print('Input Sentence : ', sent)\n",
        "  output_sent = translator(sent, input_lc, output_lc)\n",
        "  print('Output Sentence : ', output_sent)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "N8r_t6x2OQGT",
        "outputId": "2022f174-132c-44c1-f28d-fc70589cd0d2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translating from English to Hindi\n",
            "Input Sentence :  Yo! How are you?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-8d642c427b60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Translating from {input_ln} to {output_ln}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input Sentence : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0moutput_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output Sentence : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-763b4ca077ba>\u001b[0m in \u001b[0;36mtranslator\u001b[0;34m(input_sentence, input_lang_code, output_lang_code, model, tokenizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# Generate Output Tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mgenerated_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforced_bos_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang_code_to_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput_lang_code\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# Convert Tokens to Sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: ' hi_IN'"
          ]
        }
      ]
    }
  ]
}